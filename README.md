# Literary-Language-Models
It is a commonplace in the literature on Large Language Models that they are biased by their training data. This project explores one aspect of that biasing. Since GPT-1, novels and other creative writing have been included in most major transformer-based language models. What difference do they make? We test that question by revisiting the BERT model, which was trained on Wikipedia and BookCorpus. How does a model trained on both corpora behave differently than a model trained only on Wikipedia?<br><br>
This repository contains code used to (1) compare predictive accuracy on various linguistic features (morphology, part-of-speech, supersense) and (2) generate text from each model. Scripts can be found in the 'code' folder. An example workflow for each task can be found in the bash scripts: 'run_mlm.sh' and 'run_generate.sh'
Models are implemented in the [PyTorch/HuggingFace](https://github.com/huggingface/transformers/tree/main) framework. Morphological features are tagged using [SpaCy](https://spacy.io). Part-of-Speech tags and Supersense tags are from [BookNLP](https://github.com/booknlp/booknlp/tree/main).
